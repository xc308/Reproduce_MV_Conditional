---
title: "Reproduce_conditional2"
author: "XC"
date: "27/09/2021"
output:
  pdf_document:
    fig_caption: yes
csl: apa.csl
Extension: raw_tex
---
### what is \newcommand ?
ref: https://stackoverflow.com/questions/41655383/r-markdown-similar-feature-to-newcommand-in-latex

\newcommand{\Deltab} {\Delta} 
\newcommand{\intd} {\textrm{d}}
\newcommand{\Bmat} {B}
\newcommand{\Cmat} {C}
\newcommand{\cmat} {c}
\newcommand{\Imat} {I}
\newcommand{\bvec} {b}
\newcommand{\svec} {s}
\newcommand{\uvec} {u}
\newcommand{\omegab} {\omega}
\newcommand{\s}{s}
\newcommand{\h}{h}
\renewcommand{\b}{b}
\newcommand{\e}{e}
\newcommand{\z}{z}
\renewcommand{\v}{v}
\renewcommand{\u}{u}
\newcommand{\w}{w}
\renewcommand{\d}{d}
\newcommand{\Z}{Z}
\newcommand{\x}{x}
\newcommand{\Y}{Y}
\newcommand{\Yvec} {Y}
\newcommand{\Zvec}{Z}
\newcommand{\epsilonb}{\varepsilon}
\newcommand{bI} {I}
\newcommand{\bB}{B}
\newcommand{\bbeta}{\beta}
\newcommand{\bzero}{0}
\newcommand{\bSigma}{\Sigma}
\newcommand{\E}{E}
\newcommand{\cov} {\mathrm{cov}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\Gau}{\mathrm{Gau}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\T}{{ \mathrm{\scriptscriptstyle T} }}

\renewcommand{\figurename}{Fig.}


## Setting up

```{r}
#install.packages("devtools")
#library(devtools)
#install_github("andrewzm/bicon") 
```

```{r, message=FALSE}

# INLA
#install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library(INLA)
```

```{r}
# For core operation
library(Matrix)
library(dplyr)
library(tidyr)
```

```{r}
# for plotting and for arranging the figures into panels for publication
library(ggplot2)
library(gridExtra)
library(grid)
#install.packages("extrafont")
library(extrafont)
```

```{r}
# provides the data
#install.packages("maptools")
library(maptools)

#install.packages("mapproj")
library(mapproj)

library(RandomFields)
```

```{r}
# contains a handy routing for computing CRPSs, crps: Continuous Ranked Probability Score

#install.packages("verification")
library(verification)
```

```{r}
# for parallel operations we will be requiring `foreach` and `doParallel`
#install.packages("foreach")
library(foreach)
library(doParallel)
```


```{r}
library("bicon")
```


Example below consider four models that vary only through the interaction function $b_o(h)$. The models are

\begin{equation*}
\begin{array}{ll}
\textrm{Model 1 (independent Mat{\'e}rns):} &b_o(\h) \equiv 0, \\
\textrm{Model 2 (pointwise dependence):} &b_o(\h) \equiv A\delta(\h), \\
\textrm{Model 3 (diffused dependence):} & \textrm{Model 4 with~} \Deltab = 0 \\
\textrm{Model 4 (asymmetric dependence):} &b_o(\h) \equiv \left\{\begin{array}{ll} A\{1 - (\|\h - \Deltab\|/r)^2\}^2, & \| \h - \Deltab\| \le r \\ 0, & \textrm{otherwise}, \end{array} \right. 
\end{array}
\end{equation*}

where $\Deltab = (\Delta_1, \Delta_2)^\T$ is a shift-parameter vector that captures asymmetry, 
  $r$ is the aperture parameter, 
  and $A$ is a scaling parameter. 
  In Models 3 and 4, $b_o(\h)$ is a shifted bisquare function defined on $\mathbb{R}^2$. 
  The covariance functions $C_{11}(\cdot)$ and $C_{2|1}(\cdot)$ are Matérn covariance functions. 
  For each model we also consider a \emph{reversed} dependence, where we switch $Y_2$ and $Y_1$. This gives us a total of eight models to fit and compare.
  
  

First, set program options, indicating which parts of the program we want to run and which parts we want to skip

```{r}
### Model choice
model_names <- c("independent","pointwise","moving_average_delta0","moving_average")
# img_path <- "../paper/art"                  ## Where to save the figures
show_figs <- 1                              ## Show the figures in document
print_figs <- 0                             ## Print figures to file (leave =0)
LK_analysis <- 0                            ## Carry out likelihood analysis
LOO_analysis <- 0                           ## Carry out LOO analysis
Shifted_Pars_estimation <- 0                ## Fit shifted parsimonious Matern
RF_estimation <- 0                          ## Carry out LOO with RFields
useMPI <- 0                                 ## MPI backend available?
```


## The data
```{r,eval=TRUE}
data(weather,package = "RandomFields")
weather <- weather %>% data.frame()
print(head(weather))
```
The `weather` table contains four fields, with latitude, longitude, `pressure forecasting errors`, and `temperature forecasting errors` for December 13, 2003 at 4 p.m. in the North American Pacific Northwest.


Since pressure and temperature have different units, we find a scaling factor by taking the ratio of the sample variances of the two variates, and computing its square root. use this factor to scale the pressure variable


```{r}
p_scale <- var(weather$pressure) / var(weather$temperature) %>%
  sqrt() %>%
  as.numeric()

```


A function `form_Z` extract $Z_1$ and $Z_2$ and concatenate them into one long vector $Z$. If model number is > 4, then vectors $Z_1$ and $Z_2$ are inverted.

Also define `m1` as the number of observations of $Y_1$, `m2` as the number of observations of $Y_2$ and `m` as the total number of observations.

```{r}
form_Z <- function(model_num, scale = T) {
  Z1 <- weather$temperature
  Z2 <- weather$pressure
  
  if (scale) Z2 <- Z2 / p_scale   # scale press
  
  if (model_num > 4) {
     temp <- Z1
     Z1 <- Z2
     Z2 <- temp                   # reverse Z1, Z2 
  }
  Z <- rbind(Z1, Z2)
}

m1 <- m2 <- nrow(weather)        # Numb of obs of Y1, Y2
m  <- m1 + m2                    # total numb of obs
I_m1 <- diag(m1)                 # Identity matrix of m1 by m1
```


## Process discretisation

approximate the processes as a sum of elemental basis functions (tent functions) constructed on a triangulation.

The triangulation is formed using the mesher in the `INLA` package

provide a tailored function in the package `bicon`, `initFEbasis`, 
which takes information from the `INLA` mesher and casts it into a `Mesh` object

Importantly, the `Mesh` object also contains information on the areas of the 
elements in the Voronoi tesselation, which will be used to approximate the integrations.

```{r}
## Constructing mesh
##------------------

mesh <- inla.mesh.2d(loc = weather[c("lon", "lat")], 
             cutoff = 0, max.edge = 0.75, offset = 4)  # fine mesh
# Create a triangle mesh based on initial point locations


mesh_locs <- mesh$loc[, 1:2]     # in 2-D, only need lon, lat of the nodes


#str(mesh)
# $ n       : int 2071
# $ loc     : num [1:2071, 1:3] -130 -114 -111 -111 -114 ...
# $ graph   :List of 5
#  ..$ tv : int [1:3982, 1:3] 289 157 162 658 172 1275 336 1613 9 165 ...
#  ..$ vt : int [1:2071, 1] 2619 1734 3559 3375 3629 1738 3413 2356 2165 202 ...
#  ..$ tt : int [1:3982, 1:3] 2967 3869 461 3041 69 2399 3314 208 2705 22 ...
#  ..$ tti: int [1:3982, 1:3] 1 1 1 1 2 2 1 1 1 2 ...
#  ..$ vv :Formal class 'dgTMatrix' [package "Matrix"] with 6 slots
#  .. .. ..@ i       : int [1:12104] 1208 1238 1378 911 1046 1117 847 848 1943 1073 ...
#  .. .. ..@ j       : int [1:12104] 0 0 0 1 1 1 2 2 2 3 ...
#  .. .. ..@ Dim     : int [1:2] 2071 2071
#  .. .. ..@ x       : num [1:12104] 1 1 1 1 1 1 1 1 1 1 ...

```

```{r}
head(mesh_locs)
str(mesh_locs)
# num [1:2071, 1:2]
```

```{r}
head(as.matrix(mesh_locs))
str(as.matrix(mesh_locs))  # num [1:2071, 1:2]
```


```{r}
## Compute distances as in Gneiting (2010) 
# -- great circle distances
##---------------------------------------------

# str(RFearth2dist(coord = as.matrix(mesh_locs)))
# calculates distances, cf. dist, assuming that the earth is an ellipsoid

# Angle mode switches to 'degree'.
#  'dist' num [1:2143485] 1373 1662 2264 2387 2087 ...

D <- as.matrix(RFearth2dist(coord = as.matrix(mesh_locs)))

str(D)  # num [1:2071, 1:2071] 0 1373 1662 2264 2387
 
```

```{r}
str(c(D))  #  # 2071 * 2071 = 4289041
# num [1:4289041] 0 1373 1662 2264 2387 ...
Dvec <- as.double(c(D))
```


```{r}
Dobs <- as.matrix(RFearth2dist(coord = as.matrix(weather[c("lon", "lat")])))
str(Dobs) # num [1:157, 1:157] 0 697 502 502 532 ..
Dobs_vec <- as.double(c(Dobs))

```


Below is optional
```{r}
## Cast into custom Mesh object (optional) 
##-----------------------------

#install.packages("initFEbasis")
#library(initFEbasis)
#Mesh <- initFEbasis(p = mesh_locs,        # n * 2 matrix of vertex locations
#            t = mesh$graph$tv,     # m * 3 matix of triangulation; each row identify which row of p to make up construction
#            K = mesh$graph$vv     # connection matrix 
#)

```
?initFEbasis: 
initialise a finite element basis which initialises an object of class FEBasis which defines a set of elemental ‘tent’ basis functions over a pre-specified triangulation in 2-D


```{r}
#str(Mesh)
#$ vars:'data.frame':	2071 obs. of  4 variables:
#  .. .. ..$ x        : num [1:2071] -130 -114 -111 -111 -114 ...
#  .. .. ..$ y        : num [1:2071] 36.8 36.8 40.1 52.6 55.7 ...
#  .. .. ..$ n        : int [1:2071] 1 2 3 4 5 6 7 8 9 10 ...
#  .. .. ..$ area_tess: num [1:2071] 0.112 0.137 0.146 0.128 0.111 ...
```


Alternative to Mesh obj of FEbasis package, we could use `deldir` package to 1. construct Voronoi tesselation from INLA mesh points; 2. from the output summary, extract the tessaltion area directly. 

```{r}
#install.packages("deldir")
library(deldir)

Voroni <- deldir(x = mesh_locs[, 1], 
       y = mesh_locs[, 2], 
       rw = c(min(mesh_locs[, 1]) - 0.00001,
              max(mesh_locs[, 1]) + 0.00001,
              min(mesh_locs[, 2]) - 0.00001,
              max(mesh_locs[, 2]) + 0.00001))

str(Voroni)

Area.tess <- Voroni$summary$dir.area
```

```{r}
#View(deldir)

```

Next establish the dimension of our grids. 
Since we will be evaluating $Y_1$ and $Y_2$ on the same grid, `n1` = `n2`
```{r}
## Mesh Size
##-----------

n2 <- n1 <- nrow(mesh_locs)
n = n1 + n2
```


Will approximate the integration using the rectangular rule. 
When using finite elements, this reduces to using the areas of the Voronoi tessellation as integration weights.

We first compute the vector of displacements $h$ which will be of length (`n2` $\times$ `n1`).

Then, with each element we associate an integration weight equal to the area of the Voronoi tessellation of the element.


```{r}
str(mesh_locs)
str(mesh_locs[1, ])   # vector
str(t(mesh_locs) - mesh_locs[1, ])  
# num [1:2, 1:2071] 0.00 0.00 1.55e+01 

head(t(t(mesh_locs) - mesh_locs[1, ]))
```


```{r}
# Mesh Integration points
#-------------------------

h <- matrix(0, n1 * n2, ncol = 2)
Area <- rep(0, n1 * n2)
for (i in 1:n1) {
  h[((i - 1) * n1 + 1) : (i * n1), ] <- t(t(mesh_locs) - mesh_locs[i])
  Area[((i - 1) * n1 + 1) : (i * n1) ] <- Area.tess

}

str(h) # num [1:4289041, 1:2]

h1_double <- as.double(h[, 1])
h2_double <- as.double(h[, 2])
str(h1_double)
```



## Organizing the Observations
Map process to observations by constructing an incidence matrix, 
which is `1` if obs conincides with a vertex of triangualtion mesh, and `0` otherwise. 

Dimension of the whole incidence matrix is  (`m1` + `m2`) $\times$ (`n1` + `n2`), where `m1 <- m2<- nrow(weather)` 

Since in this problem we have co-located observations, we find the incidence matrix for one of the observations, $Z_1$, and then form the whole incidence matrix by simply constructing a block diagonal matrix (using `bdiag`). 

We use `left_join` to find the tri grid vertex with which obs location coincide, which returns `NA` if no coincides with vertex. 


```{r}
mesh_locs <- data.frame(lon = mesh_locs[, 1], lat = mesh_locs[, 2])
#head(left_join(x = mesh_locs, y = weather))
idx <- which(!is.na(left_join(x = mesh_locs, y = weather)$temperature))

str(idx)  # int [1:157] 9 10 11 12 13 14 15 16 17 18 ...

C1 <- sparseMatrix(i = 1:m1, j = idx, x = 1, dims = c(m1, n1)) # incidence matrix
C <- bdiag(C1, C1)  # total incidence matrix

str(C)
```



## Maximum likelihood estimation

Since the optimisation algorithm requires a parameter vector of the same length (irrespective of the model number) 

we first define a function `append_theta` that takes the parameter vector associated with the model in question and appends it so it is of the required size (in this case of length 12).

M1(M5): append 4 0's, M2(M6): append 3 0's, M3(M7): append 2 0's; 

```{r}

append_theta <- function(theta, model_num) {
  if (model_num %in% c(1, 5)) {  # M1 and M5
    theta <- c(theta, rep(0, 4))
    #theta[10] <- 0.001
  } else if (model_num %in% c(2, 6)) {
    theta <- c(theta, rep(0, 3))
    theta[10] <- 0.001
  } else if (model_num %in% c(3, 7)) {
    theta <- c(theta, rep(0, 2))
  } 
  theta
}
```


Next, we require a function that, given the parameter vector `theta` and the model number `model_num`, returns the required matrices and vectors used in fitting. These are the matrices
\begin{equation}\label{eqn:cov-matrix}
\textrm{\texttt{SY}} =  \begin{bmatrix}\bSigma_{11} & \bSigma_{11}\bB^\T \\ \bB \bSigma_{11} & \bSigma_{2\mid 1}+\bB\bSigma_{11}\bB^\T \end{bmatrix}, ~~\qquad \textrm{\texttt{So}} =  \begin{bmatrix}\tau_1^2I_m  & 0\\ 0 & \tau_2^2I_m  \end{bmatrix}.                                
\end{equation}

We then add these two together to obtain the matrix $\textrm{cov}((\Yvec_1^\T,\Yvec_2^\T)^\T)$ which, recall that for this example is identical to $\textrm{cov}((\Zvec_1^\T,\Zvec_2^\T)^\T)$ since the data is equal to the process at the observed locations. 

If `whole_mesh` is `TRUE`, then the process covariance matrix is evaluated over the entire mesh (used for cokriging at unobserved locations).


```{r}
str(h)  # num [1:4289041, 1:2]

source("bisq_fn(2d_B).R")

if (model_num %in% c(3, 4, 7, 8)) {
  B <- bisq_B(h, delta = theta[11:12], r =  theta[10], 
              A = theta[9], area = Area, 
              n1 = n1, n2 = n2)  #  num [1:2071, 1:2071]  proc lvl
}

str(C1)  # @ Dim     : int [1:2] 157 2071

C1B <- C1 %*% B  # obs * proc %*% proc * proc = obs * proc
```


```{r}
## Form matrices (scaled pressure)

source("make_cov_fn.R")

# obs level 157
C11 <- makeC(h = Dobs_vec, Var = theta[3], Kappa = theta[5], nu = theta[7])  # obs
C2_1 <- makeC(h = Dobs_vec, Var = theta[4], Kappa = theta[6], nu = theta[8]) # obs


if (Model_num %in% c(3, 4, 7, 8) | whole_mesh == T) {
  C11_proc <- makeC(Dvec, Var = theta[3], Kappa = theta[5], nu = theta[7])
  C12 <- C1B %*% C11_proc %*% t(C1)    # obs * obs
  C21 <- t(C12)
  C22 <- C2_1 + forceSymmetric(C1B %*% forceSymmetric(C11_proc) %*% t(C1B))
} else {
  C21 <- C12 <- C11 * theta[9]
  C22 <- C2_1 + C11 * theta[9]^2
}


# grid level 2071
if (whole_mesh == T) {
  C11 <- C11_proc
  C2_1_proc <- makeC(Dvec, Var = theta[4], Kappa = theta[6], nu = theta[8]) 
  C21 <- C11 %*% B
  C12 <- t(C21)
  C22 <- C2_1_proc + crossprod(chol(C11) %*% t(B))
}



## Form matrices (Unscaled pressure)
C22_true <- ifelse(Model_num < 5, p_scale^2, 1) * C22
C11_true <- ifelse(Model_num > 4, p_scale^2, 1) * C11
C12_true <- C12 * p_scale
C21_true <- C21 * p_scale

CY_true <- rbind(cbind(C11_true, C12_true), cbind(C21_true, C22_true)) %>% as("dgeMatrix")


# ref
#m1 <- m2 <- nrow(weather)        # Numb of obs of Y1, Y2
#m  <- m1 + m2                    # total numb of obs
#I_m1 <- diag(m1)                 # Identity matrix of m1 by m1

I_m1 <- diag(m1)
Co_true <- bdiag(ifelse(model_num < 5, 1, p_scale^2) * theta[1] * I_m1, 
      ifelse(model_num < 5, p_scale^2, 1) * theta[2] * I_m1)   
        # only scale theta[2],as it's related to press

if (whole_mesh == T) t(C) %*% Co_true %*% C   # proc level

list(CY = CY_true, Co = Co_true, Z = form_Z(model_num, scale = F))

```


```{r}
# ref
form_Z <- function(model_num, scale = T) {
  Z1 <- weather$temperature
  Z2 <- weather$pressure
  
  if (scale) Z2 <- Z2 / p_scale   # scale press
  
  if (model_num > 4) {
     temp <- Z1
     Z1 <- Z2
     Z2 <- temp                   # reverse Z1, Z2 
  }
  Z <- rbind(Z1, Z2)
}
```



```{r} 
# ref
h <- matrix(0, n1 * n2, 2)
areas <- rep(0, n1 * n2)
for (i in 1:n2) {
  h[((i - 1) * n1 + 1):(i * n1), ] <- t(t(mesh_locs) - mesh_locs[i, ])
  areas[((i - 1) * n1 + 1):(i * n1)] <- Mesh["area_tess"]
}

head(h)
str(mesh_locs)

```


```{r}
# now put above into one function
construct_mat <- function(theta, model_num, whole_mesh = F) {
  
  if (model_num %in% c(3, 4, 7, 8)) {
    str(h)  # num [1:4289041, 1:2]

    source("bisq_fn(2d_B).R")
    B <- bisq_B(h, delta = theta[11:12], r =  theta[10],   # h is ready above
              A = theta[9], area = Area, 
              n1 = n1, n2 = n2)  #  num [1:2071, 1:2071]  proc lvl
    
  }
    
  str(C1)  # @ Dim     : int [1:2] 157 2071
  C1B <- C1 %*% B  # obs * proc %*% proc * proc = obs * proc
  
  
  
  ## Form matrices (scaled pressure)
  source("make_cov_fn.R")
  
  
  # obs level 157
  C11 <- makeC(D = Dobs_vec, Var = theta[3], Kappa = theta[5], nu = theta[7])  # obs
  C2_1 <- makeC(D = Dobs_vec, Var = theta[4], Kappa = theta[6], nu = theta[8]) # obs
  
      # need to link btw obs and proc  
  if (model_num %in% c(3, 4, 7, 8) | whole_mesh == T) {
    C11_proc <- makeC(D = Dvec, Var = theta[3], Kappa = theta[5], nu = theta[7])
    C12 <- C1B %*% C11_proc %*% t(C1)    # obs * obs
    C21 <- t(C12)
    C22 <- C2_1 + forceSymmetric(C1B %*% forceSymmetric(C11_proc) %*% t(C1B))
  } else {
    C21 <- C12 <- C11 * theta[9]
    C22 <- C2_1 + C11 * theta[9]^2
  }
  
  
  # grid level 2071
  # no need to link btw obs and proc
  if (whole_mesh == T) {
    C11 <- C11_proc       # proc lvl
    C2_1 <- makeC(D = Dvec, Var = theta[4], Kappa = theta[6], nu = theta[8])
    C12 <- B %*% C11      # proc lvl
    C21 <- t(C12)
    C22 <- C2_1 + crossprod(chol(C11)) %*% t(B)  
  }
  
  
  ## Form matrices (Unscaled pressure)
  C22_true <- ifelse(model_num < 5, p_scale^2, 1) * C22
  C11_true <- ifelse(model_num > 4, p_scale^2, 1) * C11
  C12_true <- C12 * p_scale
  C21_true <- C21 * p_scale
  
  CY_true <- rbind(cbind(C11_true, C12_true), cbind(C21_true, C22_true)) %>% as("dgeMatrix")

  
  ## So, obs level
  I_m1 <- Imat(m1)  # Imat <- function(n) sparseMatrix(i = 1:n, j = 1:n, x = 1)
  Co_true <- bdiag(ifelse(model_num < 5, 1, p_scale^2) * theta[1] * I_m1, 
      ifelse(model_num < 5, p_scale^2, 1) * theta[2] * I_m1)   
        # only scale theta[2],as it's related to press

  if (whole_mesh == T) t(C) %*% Co_true %*% C   # proc level

  list(CY = CY_true, Co = Co_true, Z = form_Z(model_num, scale = F))
} 

```


## Loglik
- Now define the log-liklihood function:
  - The joint distribution of [Z1, Z2] ~ Gau(\bzero, \bSigma_Z), where the \bSigma_Z = CY + Co. 
  - So the logLik = -(d/2) * log(2pi) - 1/2 * log(det(\bSigma_Z)) - 1/2 * t(Z) * \bSigma_Z^(-1) * Z.

- To allow CV, drop certain obs: 
  - The indices to be dropped is stored in `i`, and if no obs drop, then `i = NULL`. 
```{r}

Loglik_Model <- function(theta, model_num, i = NULL) {

  # theta1 <- sigma2e1
  # theta2 <- sigma2e2
  # theta3 <- sigma211
  # theta4 <- sigma22_1
  # theta5 <- kappa11
  # theta6 <- kappa2_1
  # theta7 <- nu11
  # theta8 <- nu2_1
  # theta9 <- A
  # theta10 <- r
  # theta11 <- delta1
  # theta12 <- delta2
  
  
  ## append theta M1(5):append 4 0's, M2(6): append 3 0's, M3(7): append 2 0's
  theta <- append_theta(theta, model_num)
  
  
  ## hard constraint on parameters
  if (theta[1] <= 0 | theta[2] <= 0 | theta[3] <= 0 | theta[4] <= 0 |
      theta[5] <= 0.001 | theta[6] <= 0.001 |theta[7] <= 0.05 | theta[8] <= 0.05 |
      theta[10] <= 0.0005) {
          return(Inf) 
  } else {
    ## Construct Matrices
    X <- construct_mat(theta, model_num, scale = F)
    
    
    ## drop obs to CV
    if(is.null(i)) {
      CY <- X$CY
      Co <- X$Co
      Z <- X$Z
    } else {
      CY <- X$CY[-i, -i]
      Co <- X$Co[-i, -i]
      Z <- X$Z[-i, , drop = F]
    }
  
    ## Evaluate log-lik function
    cholYo <- chol(CY + Co)
    log_lik <- -(-0.5 * nrow(Z) * log(2 * pi) 
    - 0.5 * logdet(cholYo) 
    - 0.5 * t(Z) %*% chol2inv(cholYo) %*% Z) %>% as.numeric()
    
    return(log_lik)
  }
}
```


## Optimization 
  - For optimization, use `optim` (BFGS) function; 
  - 3000 iterations and set `trace = 6` for detailed output;
  - not to compute the Hessian as this is not required as not in our analysis;
  - parameter `i` indicate if the ith obs is involved in the fit, 
      and if i = NULL, all obs are included. 
  - the function `optim_loglik` is called for each model later on
  
  
```{r}  
optim_loglik <- function(par, model_num, i = NULL) {
  optim(par = par, 
        fn = Loglik_Model,
        model_num = model_num,
        i = i, 
        hessian = F,
        control = list(trace = 6, 
                       pgtol = 0, 
                       parscale = rep(0.1, length(par)), # par/parscale
                       maxit = 3000))
}  

```



## Fit the models
- The last function we need to define is one that fits all the models, 
possibly with a set of observations in `i` removed. 

- In the function `fit_all_models` below, 1st fit the Model1 using realistic 
  starting value and store the result in `fit.Model1`
- The reverse version (with pressure as $Y1$) is fitted and stored in `fit.Model1_rev`
- Model2 is fitted using the parameters of fit.Model1 as starting values, 
  and Model3 is fitted using mle of Model2 and so on. 
- The reversed Model 2 is fitted using the par of fit.Model1_rev as starting values.

```{r}

fit_all_models <- function(i) {
  fit.Model1 <- optim_loglik(par = c(0.01, 1, 5, 15, 0.01, 0.01, 0.6, 1.5), model_num = 1, i)
  fit.Model2 <- optim_loglik(par = c(fit.Model1$par, -0.2), model_num = 2, i)
  fit.Model3 <- optim_loglik(par = c(fit.Model2$par, 0.1), model_num = 3, i)
  fit.Model4 <- optim_loglik(par = c(fit.Model3$par, 0, 0), model_num = 4, i)
  
  fit.Model1_rev <- optim_loglik(par = c(1, 0.01, 15, 5, 0.01, 0.01, 1.5, 0.6), model_num = 5, i)
  fit.Model2_rev <- optim_loglik(par = c(fit.Model1_rev$par, -0.2), model_num = 6, i)
  fit.Model3_rev <- optim_loglik(par = c(fit.Model2_rev$par, 0.1), model_num = 7, i)
  fit.Model4_rev <- optim_loglik(par = c(fit.Model3_rev$par, 0, 0), model_num = 8, i)
  
  list(Model1 = fit.Model1,
       Model2 = fit.Model2,
       Model3 = fit.Model3,
       Model4 = fit.Model4,
       Model5 = fit.Model1_rev,
       Model6 = fit.Model2_rev,
       Model7 = fit.Model3_rev,
       Model8 = fit.Model4_rev)
  
}
```



## Fit all models above to data
- refer to lines 141, when LK analysis is 0, then load results from cache, 
  when 1, fit from scratch
  
```{r}  
if (LK_analysis == 1) {
  fit_all_data <- fit_all_models(i = NULL)
  save("fit_all_data", file = paste0("/Users/xchen/Dropbox/Learn/Reproduce_MV_Conditional/LK_analysis.rda"))
} else {
  load(system.file("extdata/temp_pressure", "LK.fit.rda", package = "bicon"))
}

# results from option + : π†¥øœ∑´®åß∂ƒ©˙∆˚¬…Ω≈ç√∫~ µ≤≥÷¡€#¢¢∞¶•ªº <- ≠
# getwd() 
# [1] "/Users/xchen/Dropbox/Learn/Reproduce_MV_Conditional"
```



```{r}
str(fit_all_data)

# List of 8
# $ Model1:List of 5
#  ..$ par        : num [1:8] 3.78e-07 9.12e-01 6.76 1.48e+01 1.11e-02 ...
#  ..$ value      : num 1277
#  ..$ counts     : Named int [1:2] 779 NA
# .. ..- attr(*, "names")= chr [1:2] "function" "gradient"
#  ..$ convergence: int 0
#  ..$ message    : NULL
```


## AIC
- The Akaike information criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data.
- Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. 
- provides a means for model selection.

- Formula:
  - Let k be the number of estimated parameters in the model.
  - Let L_hat be the maximum value of the likelihood function for the model
  - -2 * log(L_hat) + 2 * k


```{r}
print("(Neg log-likelihood values for each models trained on all data)")
fit_all_data$Model1$value

sapply(fit_all_data, function(x) x$value)
#   Model1   Model2   Model3   Model4   Model5   Model6   Model7   Model8 
# 1276.770 1269.922 1264.901 1258.212 1276.770 1266.826 1268.983 1268.486 


AIC <- sapply(fit_all_data, function(x) x$value) * 2 + 2 * c(8, 9, 10, 12)
AIC

#   Model1   Model2   Model3   Model4   Model5   Model6   Model7   Model8 
# 2569.541 2557.844 2549.803 2540.425 2569.541 2551.651 2557.967 2560.972 


```



## Parameter estimates output

- Since p_scale was used to put pressure on the same scale as temperature,
  now need to scale back the fitted marginal std deviations (theta[c(2, 4)]) of the pressure fields
  so that they are on the original scale. 
  
```{r}
#fit_all_data$Model1$par
#data.frame(t(fit_all_data$Model1$par))  # row vector; then df 1*8

# plyr::rbind.fill(a list of dfs, ...) rowbind all dfs and add NA to missing columns

par_est <- sapply(fit_all_data, function(x) data.frame(t(x$par))) %>%
  plyr::rbind.fill()

par_est1 <- par_est[1:4, ]
par_est1[, c(1, 3)] <- par_est1[, c(1, 3)]
par_est1[, c(2, 4)] <- par_est1[, c(2, 4)] * p_scale # scale back the marginal stdeviations
par_est1[, 9] <- par_est1[, 9] * p_scale


## latex
colnames(par_est1) <- c("$\\sigma^2_{1}$", "$\\sigma^2_{2}$", 
                        "$\\sigma^2_{11}$", "$\\sigma^2_{2|1}$",
                        "$\\kappa_{11}$", "$\\kappa_{2|1}$",
                        "$\\nu_{11}$", "$\\nu_{2|1}$", 
                        "$A$", "$r$", "$\\delta_1$", "$\\delta_2$")


rownames(par_est1) <- paste("Model", 1:4)

print(xtable::xtable(par_est1, digits = c(rep(2, 5), 3, 3, rep(2, 6))),
      sanitize.text.function = function(x) {x},
      hline.after = NULL)

# if argument 'digits' is a vector of length more than one, 
# it must have length equal to 13 ( ncol(x) + 1 )

```


## Predict

- We predict temperature and pressure fields at unobserved locations using cokriging
- Assume mean zero, so simple cokriging
- The predictive mean and variance is obtained by simple conditioning and bivariate (multi) Gau;
- One argument of the function`i`, and if `i = NULL` then data is used to predict
  all (observed and unobserved) locations;
- Otherwise, prediction is carried out at location `i` with the ith obs removed;
- Note that when `i` is specified it is assumed that only the covariance matrices associated with the observation locations are supplied.

```{r}
cokrig <- function(X, i = NULL) {
  Cz <- X$CY + X$Co
  
  if (is.null(i)) {
    # predict at all locations 2071
    # Cz at proc level
    
    Z <- X$Z
    Q <- chol2inv(chol(C %*% Cz %*% t(C))) %>% as("dgeMatrix")  # Cz inverse
    mu_pred <- (Cz %*% t(C) %*% Q %*% Z) %>% as.numeric()
    var_pred <- (diag(Cz) - Cz %*% t(C) %*% Q %*% C %*% Cz) %>% as.numeric()   
    # 2nd part is 1*1; 1st part is C00 also 1*1
    
    data.frame(mu_pred = mu_pred, var_pred = var_pred)
  } else {
    # predict only at the ith obs being removed
    # choose only those obs 156 conincide with process
    # Cz at obs lvl, nrow(weather) = 156 
    
    Cz_inv <- chol2inv(chol(Cz[-i, -i])) %>% as("dgeMatrix")
    mu_pred <- Cz[i, -i] %*% Cz_inv %*% Z[-i, , drop = F] %>% as.numeric()
    # Cz[i, -i] is [co1' co2']
    var_pred <- (diag(Cz[i, i]) - Cz[i, -i] %*% Cz_inv %*% Cz[-i, i]) %>% as.numeric()
    
    data.frame(mu_pred = mu_pred, var_pred = var_pred, 
               Z = X$Z[i, ], i = i) # extract out the ith removed one
    
  }
 
}
```



Now predict at all mesh locations using model 1 and 4
- 1st construct the required matrices X1, X4
- then carry out the cokriging and add the mean predictions to the mesh
```{r}

X1 <- construct_mat(theta = append_theta(theta = fit_all_data[[1]]$par, model_num = 1),
                    model_num = 1, whole_mesh = T)

X4 <- construct_mat(theta = append_theta(theta = fit_all_data[[1]]$par, model_num = 4), 
                    model_num = 4, whole_mesh = T)


#ALL1 <- cokrig(X = X1, i = NULL)
#ALL4 <- cokrig(X = X4, i = NULL)
```





## Question??

```{r}
Matern_fn <- function(phi = 1, Alpha = 1, nu = 0.5, d) {
  if (any(d) < 0) 
  stop("distance d must be non-neg.")
  
  d <- Alpha * d
  d[d == 0] <- 1e-6   # avoid sending exact 0 to basselK
  
  const <- 2^(nu - 1) * gamma(nu)
  const <- 1/const
  
  X <- phi * const * (d)^nu * besselK(d, nu)  # a vector
  matrix(X, nrow = sqrt(length(X)))
  
}

M_results <- Matern_fn(phi = fit_all_data[[1]]$par[3], 
          Alpha = fit_all_data[[1]]$par[5],
          nu = fit_all_data[[1]]$par[7],
          d = Dvec)


str(M_results) # num [1:2071, 1:2071]

isSymmetric(M_results)  # [1] TRUE
is.positive.definite(M_results)  # [1] TRUE
Cross_M <- crossprod(chol(M_results))
```

```{r}
# incorperate Matern_fn in a new function makeC

makeC <- function(Var, Kappa, nu, D) {
  if (nu == 3/2) {
    C <- Matern_fn_32(phi = Var, Alpha = Kappa, nu)
  } else if (nu == 1/2){
    C <- Matern_fn_12 (phi = Var, Alpha = Kappa, d = D)
  } else {
    C <- Matern_fn(phi = Var, Alpha = Kappa, nu = nu, d = D)
  }
  C
}


C11_proc <- makeC(Var = fit_all_data[[1]]$par[3], Kappa = fit_all_data[[1]]$par[5], nu = fit_all_data[[1]]$par[7], D = Dvec)
# Warning in gamma(nu) : NaNs produced


isSymmetric(C11_proc )  # [1] TRUE
is.positive.definite(C11_proc) # No
```




## Leave-one-out cross validation






## The shifted parsimonious Matérn model

Repeat the analysis above for the *shifted* parsimonious Matérn model,
obtained by applying the method of @Li_2011 to the standard parsimonious model.
First we define a function that constructs the matrices based on the usual parameters.

View(cor)













