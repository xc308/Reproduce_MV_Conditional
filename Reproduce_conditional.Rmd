---
title: "Reproduce conditional approach"
author: "XC"
date: "6/16/2021"
output:
  pdf_document:
    fig_caption: yes
csl: apa.csl
Extension: raw_tex

---

```{r load-packages, include=FALSE}
library(INLA)

# for core operation
library(dplyr)
library(tidyr)
library(Matrix)

library(ggplot2)
library(gridExtra)
library(grid)
library(extrafont)

library(maptools)
library(mapproj)
library(RandomFields)

library(verification)

library(foreach)
library(doParallel)
install.packages("Rmpi")
install.packages("doMPI")
library(Rmpi)
library(doMPI)


library(bicon)
```



\newcommand{\Deltab} {\Delta}
\newcommand{\intd} {\textrm{d}}
\newcommand{\Bmat} {B}
\newcommand{\Cmat} {C}
\newcommand{\cmat} {c}
\newcommand{\Imat} {I}
\newcommand{\bvec} {b}
\newcommand{\svec} {s}
\newcommand{\uvec} {u}
\newcommand{\omegab} {\omega}
\newcommand{\s}{s}
\newcommand{\h}{h}
\renewcommand{\b}{b}
\newcommand{\e}{e}
\newcommand{\z}{z}
\renewcommand{\v}{v}
\renewcommand{\u}{u}
\newcommand{\w}{w}
\renewcommand{\d}{d}
\newcommand{\Z}{Z}
\newcommand{\x}{x}
\newcommand{\Y}{Y}
\newcommand{\Yvec}{Y}
\newcommand{\Zvec}{Z}
\newcommand{\epsilonb}{\varepsilon}
\newcommand{\bI}{I}
\newcommand{\bB}{B}
\newcommand{\bbeta}{\beta}
\newcommand{\thetab}{\theta}
\newcommand{\bzero}{0}
\newcommand{\bSigma}{\Sigma}
\newcommand{\E}{E}
\newcommand{\cov}{\mathrm{cov}} 
\newcommand{\var}{\mathrm{var}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\Gau}{\mathrm{Gau}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\T}{{ \mathrm{\scriptscriptstyle T} }}
\renewcommand{\figurename}{Fig.}

## Setting up

\begin{equation*}
\begin{array}{ll}
\textrm{Model 1 (independent Mat{\'e}rns):} &b_o(\h) \equiv 0, \\
\textrm{Model 2 (pointwise dependence):} &b_o(\h) \equiv A\delta(\h), \\
\textrm{Model 3 (diffused dependence):} & \textrm{Model 4 with~} \Deltab = 0 \\
\textrm{Model 4 (asymmetric dependence):} &b_o(\h) \equiv \left\{\begin{array}{ll} A\{1 - (\|\h - \Deltab\|/r)^2\}^2, & \| \h - \Deltab\| \le r \\ 0, & \textrm{otherwise}, \end{array} \right. 
\end{array}
\end{equation*}


where $\Deltab = (\Delta_1, \Delta_2)^\T$ is a shift-parameter vector that captures asymmetry,
$r$ is the aperture parameter,
and $A$ is a scaling parameter.

In Models 3 and 4, $b_o(\h)$ is a shifted bisquare function defined on $\mathbb{R}^2$.

The covariance functions $C_{11}(\cdot)$ and $C_{2|1}(\cdot)$ are Matérn covariance functions. 


For each model we also consider a \emph{reversed} dependence, where we switch $Y_2$ and $Y_1$. This gives us a total of eight models to fit and compare.


```{r}
### Model choice

model_names <- c("independent","pointwise","moving_average_delta0","moving_average")
image_path <- "../paper/art"
show_figs <- 1              ## show figs in document
print_figs <- 0             ## Print figures to file (leave =0)
LK_analysis <- 0            ## log-likelihood analysis 
LOO_analysis <- 0           ## LOO analysis 
Shifted_Pars_estimation <- 0  ## Fit shifted parimonious Matern
RF_estimation <- 0            ## Carry out LOO with RFields
useMPI <- 0                                 ## MPI backend available?
```



## The data
The data were made available through the package \texttt{RandomFields}.
We first load the data

```{r}
data(weather, package = "RandomFields")
weather <- weather %>% data.frame()
weather %>% head(4) %>% print()
```

The `weather` table contains four fields, with latitude, longitude, pressure forecasting errors, and temperature forecasting errors for December 13, 2003 at 4 p.m. in the North American Pacific Northwest.

Since pressure and temperature have different units, we find a scaling factor by taking the ratio of the sample variances of the two variates, and computing its square root.

We will use this factor to scale the pressure variable.

```{r}
P_scale <- var(weather$pressure) / var(weather$temperature) %>% 
  sqrt() %>%
  as.numeric()
```
  

From this data frame we extract $Z_1$ and $Z_2$ and concatenate them into one long vector $Z$ through a function `form_Z`.

The vectors $Z_1$ and $Z_2$ are inverted if the model being analysed is greater than 4 (reversed model).

We also define `m1` as the number of observations of $Y_1$, `m2` as the number of observations of $Y_2$ and `m` as the total number of observations.

```{r}
form_Z <- function(model_num, scale = T){
  Z1 <- matrix(weather$temperature)
  Z2 <- matrix(weather$pressure)
  
  if(scale) Z2 <- Z2 / p_scale   # scale pressure
  
  if(model_num > 4) {
    temp <- Z1   # move temperature values out of Z1 name into temp 
    Z1 <- Z2     # pressure values go into name Z1
    Z2 <- temp   # move original temperature values into Z2
  }
  
  Z <- rbind(Z1, Z2)  # matrix
}


## Number of observations
m1 <- nrow(weather)
m2 <- nrow(weather)
m = m1 + m2
I_m1 <- Diagonal(m1)
```


## Process Discretisation

We approximate the processes as a sum of elemental basis functions (tent functions) constructed on a triangulation.

The triangulation is formed using the mesher in the `INLA` package,
while we provide a tailored function in the package `bicon`, `initFEbasis`


?initFEbasis: 
initialise a finite element basis which initialises an object of class FEBasis which defines a set of elemental ‘tent’ basis functions over a pre-specified triangulation in 2-D



which takes information from the `INLA` mesher and casts it into a `Mesh` object

We provide several methods associated with the `Mesh` class which will be useful for plotting later on.

Importantly, the `Mesh` object also contains information on the areas of the elements in the Voronoi tesselation, which will be used to approximate the integrations.


```{r, warning=F, message=F}
## discretizing process Y1,Y2 using triangular grid

## constructing mesh
mesh <- inla.mesh.2d(loc = weather[c("lon", "lat")],
             cutoff = 0,        # minimum allowed dist between pts, two pts further apart at most of this value will be replaced by single point
             max.edge = 0.75,   # the largest allowed tri edge length
             offset = 4)        # automatic extension dist

mesh_locs <- mesh$loc[, 1:2]  # [1:2071, 1:2]


## compute distances as in Gneiting(2010) -- greate-circle distance

### Greate circle distance: shortest distance between two points on the surface of a sphere, measured along the surface of the sphere not interior

d <- RFearth2dist(as.matrix(mesh_locs)) # transform coods from earth (ellipsoid) to cartesian
D <- as.matrix(d)
Dvec <- as.double(c(D))  

```


### understandings 1
```{r}

str(mesh)
head(mesh$loc, 3)
tail(mesh$loc, 3)

d <- RFearth2dist(as.matrix(mesh_locs))
d_matrx <- as.matrix(d)
rm(d_matrx)
rm(d)

dim(D) #2071 2071
```

```{r}
length(Dvec) # 2071 * 2071 = 4289041
```


```{r}
## obseration locs distance in cartesian
Dobs <- as.matrix(RFearth2dist(as.matrix(weather[c("lon", "lat")])))

Dobsvec <- c(Dobs)
```


#### understandings 2
```{r}

obs_locs <- weather[c("lon", "lat")] ## df 157 obs, 2 vars
str(obs_locs)

obs_locs <- as.matrix(weather[c("lon", "lat")]) # [1:157, 1:2]

length(Dobs_vec)  # 24649
```



```{r}
## Cast into custom Mesh object
## define a set of tent basis functions over a prespecified triangulation in 2D
Mesh <- initFEbasis(p = mesh_locs,   # n * 2 matrix of vertex locations
            t = mesh$graph$tv,       # n * 3 matrix of triangulation; each row identifies which row of p make up tri
            K = mesh$graph$vv)       # connectivity matrix 

str(Mesh)

#head(str(Mesh))
```



## Establish the dimension of our grids

Since we will be evaluating $Y_1$ and $Y_2$ on the same grid, `n1` = `n2`.


```{r}
## Mesh size
n1 <- nrow(mesh_locs)   # 2071
n2 <- nrow(mesh_locs)
n <- n1 + n2            # 4142
``` 


As in the first document (simulation example in Section 3.2), we will approximate the integration using the rectangular rule.

When using finite elements, this reduces to using the areas of the Voronoi tessellation as integration weights.


We first compute the vector of displacements $h$ which will be of length (`n2` $\times$ `n1`) 


```{r}
## mesh integration points
h <- matrix(0, n1 * n2, 2)
areas <- rep(0, n1 * n2)
for (i in 1:n2) {
  h[((i - 1) * n1 + 1):(i * n1), ] <- t(t(mesh_locs) - mesh_locs[i, ])
  areas[((i - 1) * n1 + 1):(i * n1)] <- Mesh["area_tess"]
}

h1_double <- as.double(h[, 1])
h2_double <- as.double(h[, 2])

```

The displacements (`h1`,`h2`) and the areas `areas` will be used to construct the matrix `B` using the function `bisquare_B`.


## Organising the observations
In order to map the process to the observations we construct an incidence matrix, which contains a `1` wherever the observation coincides with a vertex on the triangulation and a `0` otherwise. 


The dimension of this incidence matrix is (`m1` + `m2`) $\times$ (`n1` + `n2`), where `m1`, `m2`, are the number of observations in $Z_1$, $Z_2$.


Since in this problem we have co-located (shared) observations, we find the incidence matrix for one of the observations, $Z_1$, 

and then form the whole incidence matrix by simply constructing a block diagonal matrix (using `bdiag`).  
We find the points with which the observation locations coincide by using the function `left_join`, which returns an `NA` if no observation coincides with the vertex.


```{r}
mesh_locs <- data.frame(lon = mesh_locs[, 1], lat = mesh_locs[, 2])

indx <- which(!is.na(left_join(mesh_locs, weather)$temperature)) # index of coincidence

length(indx)                   # 157

C1 <- sparseMatrix(i = 1:m1, j = indx, 
                   x = 1, dims = c(m1, n1))  # incidence matrix of Z1

C <- bdiag(C1, C1)    # incidence matrix of Z1, Z2
```



## Maximum likelihood estimation

Since the optimisation algorithm requires a parameter vector of the same length (irrespective of the model number)

we first define a function `append_theta` that takes the parameter vector associated with the model in question and appends it so it is of the required size (in this case of length 12).


```{r,message=F}
append_theta <- function(theta,model_num) {
  if(model_num %in% c(1,5)) {
    theta <- c(theta,rep(0,4))
    theta[10] <- 0.001
  } else if(model_num %in% c(2,6)) {
    theta <- c(theta,rep(0,3))
    theta[10] <- 0.001
  } else if(model_num %in% c(3,7)) {
    theta <- c(theta,rep(0,2))
  }
  theta
}
```


the <- c(0, 3, 2.3)
c(the, rep(0, 4))


Next, we require a function that, given the parameter vector `theta` and the model number `model_num`, returns the required matrices and vectors used in fitting. These are the matrices
\begin{equation}\label{eqn:cov-matrix}
\textrm{\texttt{SY}} =  \begin{bmatrix}\bSigma_{11} & \bSigma_{11}\bB^\T \\ \bB \bSigma_{11} & \bSigma_{2\mid 1}+\bB\bSigma_{11}\bB^\T \end{bmatrix}, ~~\qquad \textrm{\texttt{So}} =  \begin{bmatrix}\tau_1^2I_m  & 0\\ 0 & \tau_2^2I_m  \end{bmatrix}.                                
\end{equation}


We then add these two together to obtain the matrix $\textrm{cov}((\Yvec_1^\T,\Yvec_2^\T)^\T)$ which, recall that for this example is identical to $\textrm{cov}((\Zvec_1^\T,\Zvec_2^\T)^\T)$ since the data is equal to the process at the observed locations. 

If `whole_mesh` is `TRUE`, then the process covariance matrix is evaluated over the entire mesh (used for cokriging at unobserved locations).



```{r}
construct_mats <- function(theta, model_num, whole_mesh = F) {
  
  nu1 <- theta[7]
  nu2 <- theta[8]
  
  B <- theta[9] * Diagonal(n1)  # proc level
  
  
  if (model_num %in% c(3, 4, 7, 8)) {
    B <- theta[9] * bisquare_B(h1 = h1_double, h2 = h2_double, 
                               delta = theta[11:12],
                               r = theta[10],
                               n1 = n1, n2 = n2, 
                               areas = areas)
    
  }
  C1B <- C1 %*% B
  
  
  ## Form matrices (scaled Pressure variable)
  S11 <- makeS(r = Dobsvec, var = theta[3], kappa = theta[5], nu = theta[7])
  S2_1 <- makeS(r = Dobsvec, var = theta[4], kappa = theta[6], nu = theta[8])
  
  if (model_num %in% c(3, 4, 7, 8) | whole_mesh == T) {
    S11_big <- makeS(r = Dvec, var = theta[3], kappa = theta[5], nu = theta[7])
    #S21 <- C1B %*% (S11_big %*% t(C1))  # his code S21
    #S12 <- t(S21)
    S12 <- (C1 %*% S11_big) %*% t(C1B)
    S21 <- t(S12)
    S22 <- S2_1 + forceSymmetric(C1B %*% forceSymmetric(S11_big) %*% t(C1B))
  } else {
    S12 <- S21 <- S11 * theta[9]
    S22 <- S2_1 + S11 * theta[9]^2
  }

  
  if (whole_mesh == T) {
    S11 <- S11_big
    S2_1 <- makeS(r = Dvec, var = theta[4], kappa = theta[6], nu = theta[8])
    S21 <- B %*% S11_big
    S12 <- t(S21)
    S22 <- S2_1 + crossprod(chol(S11_big) %*% t(B))  
  }
  

  
  # Form matrices (Unscaled)
  S11_true <- ifelse(model_num < 5, 1, P_scale^2) * S11
  S12_true <- P_scale * S12
  S21_true <- P_scale * S21
  S22_true <- ifelse(model_num < 5, P_scale^2, 1) * S22
  
  SY_true <- rbind(cbind(S11_true, S12_true), cbind(S21_true, S22_true)) %>% as("dgeMatrix")
 
  
  I_m1 <- Imat(m1)
  So11_true <- ifelse(model_num < 5, 1, P_scale^2) * theta[1] * I_m1
  So22_true <- ifelse(model_num < 5, P_scale^2, 1) * theta[2] * I_m1
  So_true <- bdiag(So11_true, So22_true)

  if (whole_mesh == T) {
    So_true <- t(C) %*% So_true %*% C
  }
 
  list(SY = SY_true, So = So_true, Z = form_Z(model_num, scale = F))
  
}
```




his code
```{r}
construct_mats <- function(theta,model_num,whole_mesh=F) {
  
  nu1 <- theta[7]
  nu2 <- theta[8]
  
  B <- theta[9]*Diagonal(n1) # Automatically zero if Model 1
  
  if(model_num %in% c(3,4,7,8)) {
    B <- theta[9]*bisquare_B(h1_double,h2_double,
                             delta=theta[11:12], # Zero for Model with no shift
                             r=theta[10],
                             n1 = n1,
                             n2 = n2,
                             areas = areas)
  }
  C1B <- C1 %*% B
  
  
  ## Form matrices (scaled pressure)
  S11 <- makeS(r = Dobsvec,var = theta[3],
                 kappa = theta[5],nu = nu1)
  S2_1 <- makeS(r = Dobsvec,var = theta[4],
                  kappa = theta[6],nu = nu2)
  if(model_num %in% c(3,4,7,8) | whole_mesh==TRUE) {
    S11_big <- makeS(r = Dvec,var = theta[3],
                     kappa = theta[5],nu = nu1)
    S21 <- C1B %*% (S11_big %*% t(C1))
    S12 <- t(S21)
    S22 <- S2_1 + forceSymmetric(C1B %*% forceSymmetric(S11_big) %*% t(C1B))
  } else {
    S21 <- S12 <- theta[9]*S11 
    S22 <- S2_1 + theta[9]^2 * S11
  }
  
  
  
  if(whole_mesh) {
    S11 <- S11_big
    S2_1 <- makeS(r = Dvec,var = theta[4], kappa = theta[6],nu =nu2)
    S21 <- B %*% S11_big
    S12 <- t(S21)
    S22 <- S2_1 + Matrix::crossprod(chol(S11_big) %*% t(B))
  }
  
  
  ## Form matrices (Unscaled pressure)
  S11_true <- ifelse(model_num > 4,P_scale^2,1) * S11
  S12_true <- P_scale * S12
  S21_true <- P_scale * S21
  S22_true <-  ifelse(model_num < 5, P_scale^2,1) *S22
  SY_true <- rbind(cbind(S11_true,S12_true),
                   cbind(S21_true,S22_true)) %>% as("dgeMatrix")
  
  So_true <- bdiag(ifelse(model_num < 5,1,P_scale^2) * theta[1]*I_m1,
                   ifelse(model_num < 5,P_scale^2,1) * theta[2]*I_m1)
  
  if(whole_mesh) So_true <- t(C) %*% So_true %*% C
 
  
  list(SY = SY_true, So = So_true, Z = form_Z(model_num,scale=F))
}
```


Now we're in place to define the log-likelihood function. 
This is the usual Gaussian log-likelihood function. 
In the function we allow the dropping of certain observations for cross-validation purposes. 
The indices of the observations we wish to drop are stored in the parameter `i`. If `i = NULL` then no observations are dropped. This argument is useful for cross-validation.

```{r}
loglik_Model <- function(theta, model_num, i = NULL) {
  # theta1: sigma2e1
  # theta2: sigma2e2
  # theta3: sigma211
  # theta4: sigma22_1
  # theta5: kappa11
  # theta6: kappa2_1
  # theta7: nu1
  # theta8: nu2_1
  # theta9: A
  # theta10: r  aperture in interaction function not distance btw obs
  # theta11: d1
  # theta12: d2
  

  #theta <- append_theta(theta, model_num)
  theta <- append_theta(theta,model_num)
 
  
  ## Hard constraints on parameters
  if(theta[1] <= 0 |  theta[2] <= 0 | theta[3] <= 0 | 
     theta[4] <= 0 | theta[5] <= 0.001 | theta[6] <= 0.001 | 
     theta[7] <= 0.05 | theta[8] <= 0.05 | theta[10] < 0.0005) {
       return(Inf)
     } else {
    
    ## construct matrices
    X <- construct_mats(theta, model_num)
  
    
   ## drop obs for CV my code
    if (is.null(i)) {
      SY <- X$SY
      So <- X$So
      Z <- X$Z
    } else {
      SY <- X$SY[-i, -i]
      So <- X$So[-i, -i]
      Z <- X$Z[-i, , drop = F]
    }
    
    
    ## Evaluate log-likelihood function
    cholYo<- chol(SY + So)
    loglik <- -(-0.5 * logdet(cholYo) 
                -0.5 * t(Z) %*% chol2inv(cholYo) %*% Z
                -0.5 * nrow(Z) * log(2 * pi)) %>% as.numeric()
    
    return(loglik)

   }
}
```



his code
```{r}
loglik_Model <- function(theta,model_num,i=NULL) {
  # theta1:  sigma2e1
  # theta2:  sigma2e2
  # theta3:  sigma211
  # theta4:  sigma22_1
  # theta5:  kappa11
  # theta6:  kappa2_1
  # theta7:  nu11
  # theta8:  nu2_1
  # theta9:  A
  # theta10: r
  # theta11: d1
  # theta12: d2

  theta <- append_theta(theta,model_num)
  
  
  ## Hard constraints on parameters
  if(theta[1] <= 0 |  theta[2] <= 0 | theta[3] <= 0 | 
     theta[4] <= 0 | theta[5] <= 0.001 | theta[6] <= 0.001 | 
     theta[7] <= 0.05 | theta[8] <= 0.05 | theta[10] < 0.0005) {
       return(Inf)
     } else {
       
       ## Construct matrices
       X <- construct_mats(theta,model_num)
       
       ## Drop observations if required for CV
       if(is.null(i)) {  
         SY <- X$SY
         So <- X$So
         Z <- X$Z
       } else {
         SY <- X$SY[-i,-i]
         So <- X$So[-i,-i]
         Z <- X$Z[-i,,drop=F]
       }
       
       ## Evaluate log-likelihood function
       cholYo <- chol(SY + So)
       loglik <- 
         -(-0.5 * logdet(cholYo) -
             0.5 * t(Z) %*% chol2inv(cholYo) %*% Z -
             0.5 * nrow(Z)*log(2*pi)) %>% as.numeric()
       
       return(loglik)
     }
} 
```



For optimising we will use the `R` function `optim` (BFGS). 
We allow for 3000 maximum iterations and set `trace = 6` for detailed output. We choose not to compute the Hessian since this is not required in our analysis. 
Recall that the parameter `i` here contains the indices of the observations we do not wish to include in the fit. If `i = NULL` then all observations are inlcuded. The function `optim_loglik` is called for each model in the program later on.


```{r}
optim_loglik <- function(par, model_num, i = NULL) {
  optim(par = par, 
        fn = loglik_Model, 
        i = i,
        model_num = model_num,
        hessian = F, 
        control = list(trace = 6, 
                       pgtol = 0,   # projected gradient, help control convergence
                       parscale = rep(0.1, length(par)),  # Optimiz is performed on par/parscale 
                       maxit = 3000 ))  # iteration
  
}
```



The last function we need to define is one that fits all the models, 
possibly with a set of observations in `i` removed. 

In the function `fit_all_models` below, we first fit Model 1 using realistic starting values and store the results in `fit.Model1`, and then we fit the reversed version (with pressure as $Y_1$) and store that in `fit.Model1_rev`. 
Model 2 is then fit using the estimates of Model 1 as starting values. 
Model 3 uses the maximum likelihood estimates of Model 2 as starting values, and so on. 
The reversed version of Model 2 uses the results of the reversed version of Model 1 as starting values and so on.


```{r}
fit_all_models <- function(i) {
  fit.Model1  <- optim_loglik(c(0.01, 1, 5, 15, 0.01, 0.01, 0.6, 1.5), model_num = 1, i)
  fit.Model2 <- optim_loglik(c(fit.Model1$par, -0.2), model_num = 2, i)
  fit.Model3 <- optim_loglik(c(fit.Model2$par, 0.1), model_num = 3, i)
  fit.Model4 <- optim_loglik(c(fit.Model3$par, 0, 0), model_num = 4, i)
  
  fit.Model1_rev <- optim_loglik(c(1, 0.01, 15, 5, 0.01, 0.01, 1.5, 0.6), model_num = 5, i)
  fit.Model2_rev <- optim_loglik(c(fit.Model1_rev$par, -0.2), model_num = 6, i)
  fit.Model3_rev <- optim_loglik(c(fit.Model2_rev$par, 0.1), model_num = 7, i)
  fit.Model4_rev <- optim_loglik(c(fit.Model3_rev$par, 0, 0), model_num = 8, i)
  
  
  list(Model1 = fit.Model1, 
       Model2 = fit.Model2, 
       Model3 = fit.Model3,
       Model4 = fit.Model4, 
       Model5 = fit.Model1_rev,
       Model6 = fit.Model2_rev,
       Model7 = fit.Model3_rev, 
       Model8 = fit.Model4_rev)
  
}
```


try out using model1
```{r}
fit.Model1  <- optim_loglik(par = c(0.01, 1, 5, 15, 0.01, 0.01, 0.6, 1.5), model_num = 1, i = NULL)
fit.Model2 <- optim_loglik(c(fit.Model1$par, -0.2), model_num = 2, i = NULL)
fit.Model3 <- optim_loglik(c(fit.Model2$par, 0.1), model_num = 3, i = NULL)
fit.Model4 <- optim_loglik(c(fit.Model3$par, 0, 0), model_num = 4, i)

```



With all functions in place we now call `fit_all_data <- fit_all_models(NULL)`. 
All this does is fit all the models using all the observations (since `i = NULL`). 
If `LK_analysis = 1` then this is done from scratch (takes about 30 minutes), otherwise the data is loaded from cache.

```{r}
if (LK_analysis = 1) {
  fit_all_data <- fit_all_models(NULL)
  save(fit_all_data, file = paste0("./Results/fit_all_data.rda"))
  #save(fit_all_data, file = paste0("../inst/extdata/temp_pressure/LK_fits.rda"))
} else {
  load(system.file("extdata/temp_pressure", "LK_fits.rda", package = "bicon"))
}
```



```{r}
fit_all_data <- fit_all_models(NULL)
```

```{r}
str(fit_all_data)

```

The log-likelihoods and AICs given by our fit are given in the table below. Recall that Model 5 is Model 1 reversed (i.e., with pressure as $Y_1$), Model 6 is Model 2 reversed, and so on.


```{r}
print("Log_likihood for all models trained with complete dataset")
sapply(fit_all_data, function(x) x$value) # Neg LL value
sapply(fit_all_data, function(x) x$value) * 2 + 2 * c(8, 9, 10, 12) ## AIC
```


Parameter estimates can be printed using code below.
Note that since `P_scale` was used to put pressure on the same scale as temperature, we scale back the fitted marginal standard deviation of the pressure fields so that they are on the original scale.
```{r, results = "hide"}
#str(fit_all_data$Model1$par)  # a vector
#t(fit_all_data$Model1$par)   # row vector

par_est <- sapply(fit_all_data, function(x) data.frame(t(x$par))) %>%
  plyr::rbind.fill()
par_est1 <- par_est[1:4, ]
par_est1[, c(1, 3)] <- par_est1[, c(1, 3)]
par_est1[, c(2, 4)] <- par_est1[, c(2, 4)] * P_scale    # scale back
par_est1[, 9] <- par_est1[, 9] * P_scale


# tables in Latex
colnames(par_est1) <- c("$\\sigma_1$", "$\\sigma_2$", 
                        "$\\sigma_{11}$", "$\\sigma_{2|1}$",
                        "$\\kappa_{11}$", "$\\kappa_{2|1}$",
                        "$\\nu_{11}$", "\\nu_{2|1}",
                        "$A$", "$r$", "$\\delta_1$", "$\\delta_2$")

rownames(par_est1) <- c("Model1", "Model2", "Model3", "Model4")

print(xtable::xtable(par_est1, digits = c(rep(2, 5), 3, 3, rep(2, 6))),
      sanitize.text.function = function(x) {x},
      hline.after = NULL)
```


## Prediction

We predict the temperature and pressure fields at the unobserved locations using cokriging. Since we assume mean zero, this is simple cokriging; the predictive mean and variance can thus be obtained by simple conditioning with a joint multivariate Gaussian distribution. 

If `i = NULL` then the data is used to predict at all (observed and unobserved) locations. Otherwise prediction is only carried out at the locations in `i` with the observations in `i` removed. Note that when `i` is specified it is assumed that only the covariance matrices associated with the observation locations are supplied. This enables us to use the same function for cross-validation (see below).

```{r}
cokrig <- function(X, i = NULL) {
  SS <- X$SY + X$So

  if (is.null(i)) {
  # predict at all locations 2701; 
  #SS is proc lvl
  
    Z <- X$Z
    Q <- chol2inv(chol(C %*% SS %*% t(C))) %>% as("dgeMatrix")  # obs lvl
    mu_pred <- SS %*% t(C) %*% Q %*% Z %>% as.numeric()  # SS %*% t(C) = cov(Y,Z) = Co
    #var_pred <- diag(SS - SS %*% t(C) %*% Q %*% C %*% SS) %>% as.numeric()
    #var_pred <- diag(SS - SS %*% t(C) %*% Q %*% C %*% SS) %*% as.numeric()
    
    data.frame(mu_pred = mu_pred)
    #data.frame(mu_pred = mu_pred, var_pred = var_pred)
  } else {
    # predict at i which is removed from obs; 
    # proc = obs = 157
    # SS obs lvl
    
      SSinv <- chol2inv(chol(SS[-i, -i])) %>% as("dgeMatrix")
      
      #[t(C11), t(C12)] is just the SS(obs lvl) now and just need drop the diag variance
      mu_pred  <- SS[i, -i] %*% SSinv %*% X$Z[-i,, drop = F] %>% as.numeric()
      var_pred <- diag(SS[i, i] - SS[i, -i] %*% SSinv %*% SS[-i, i]) %>% as.numeric()
      
      data.frame(mu_pred = mu_pred, var_pred = var_pred,
                 Z = X$Z[i, ], i = i)
  }
}

```


Below we predict at all the mesh locations using Model 1 and Model 4.
First we construct the required matrices and store them in `X1` and `X4`.
Then we carry out cokriging and add the mean predictions to the mesh.

```{r}
remove(construct_mats)
```



```{r, message=F}

X1 <- construct_mats(theta = append_theta(fit_all_data[[1]]$par, model_num = 1),
                     model_num = 1, whole_mesh = T)
X4 <- construct_mats(theta = append_theta(fit_all_data[[4]]$par, model_num = 4),
                     model_num = 4, whole_mesh = T)


ALL1 <- cokrig(X = X1, i = NULL)
ALL4 <- cokrig(X = X4, i = NULL)
```




```{r}

Mesh["y_1Model1"] <- ALL1$mu_pred[1:n1]
Mesh["y_2Model1"] <- ALL1$mu_pred[-(1:n1)]

Mesh["y_1Model4"] <- ALL4$mu_pred[1:n1]
Mesh["y_2Model4"] <- ALL4$mu_pred[-(1:n1)]

str(Mesh)
```


## Leave-one-out cross validation

carry out leave-one-out cross validation (LOOCV) without re-fitting the model each time.

If we have an MPI cluster available we carry out the LOOCV over MPI, otherwise we parallelise using the computer's multiple cores.

The two loops below iterate over the observations and models.

```{r}
#if (LOO_analysis) {

  library(doParallel)
  cl <- parallel::makeCluster(4, setup_strategy = "sequential")
  #cl <- makePSOCKcluster(4, outfile = "cores_output.txt")
  registerDoParallel(cl)
  
  
  ## loop over each obs location
  pred <- foreach(i = 1:m1, .combine = "rbind", 
          .packages = c("Matrix", "bicon", "dplyr", "foreach")) %dopar% {
            
            fit.Model <- fit_all_data  # with fitted pars so can predict
            
            ## loop over each model (not parallised)
            pred <- foreach(j = seq_along(fit.Model), .combine = "rbind") %do% {
              
              ## construct matrices X
              X <- construct_mats(theta = append_theta(fit.Model[[j]]$par, 
                                                       model_num = j), 
                                  model_num = j)
              
              ## cokrig, leaving out the ith obs for both press and temp
              cbind(cokrig(X, i = c(i, m1 + i)), model_num = j)
            }
            
            pred 
          }
  
  
  stopCluster(cl)
  
#}


#ref:https://stackoverflow.com/questions/62730783/error-in-makepsockclusternames-spec-cluster-setup-failed-3-of-3-work    
  
```

```{r}
save(pred, file = paste0("./Results/all_predictions.rda"))
```



## The shifted parsimonious Matérn model

In this section we repeat the analysis above for the *shifted* parsimonious Matérn model,
obtained by applying the method of @Li_2011 to the standard parsimonious model.
First we define a function that constructs the matrices based on the usual parameters.

```{r}
sh_pars_mats <- function(theta) {
  
## Now create the shifted locations for the cross-covariances
    new_locs <- weather[,3:4] + 
      matrix(theta[1:2], ncol=2, nrow=nrow(weather), byrow=TRUE)

    X <- rbind(new_locs,weather[,3:4])
    tot_D <- as.matrix(RFearth2dist(as.matrix(X))) # 314:314
    D12 <- tot_D[-(1:m1),1:m1]
    D21 <- t(D12)
    
    # parsimonious Matern by Gneiting 2010 
    SY11 <- makeS(r = Dobs_vec, var = theta[9], kappa = theta[3], nu = theta[4])
    SY22 <- makeS(r = Dobs_vec, var = theta[10], kappa = theta[3], nu = theta[5])
    SY12 <- makeS(r = D21, var = theta[6] * sqrt(theta[9] * theta[10]), 
                  kappa = theta[3], nu = 0.5 * (theta[4] + theta[5])) 
    SY21 <- t(SY12)
    SY <- cbind(rbind(SY11, SY12), rbind(SY21, SY22))
    
    So11 <- theta[7]^2 * diag(m1)
    So22 <- theta[8]^2 * diag(m1)
    So   <- bdiag(So11, So22)
    
    list(SY = SY, So = So, Z = form_Z(1L, scale = F))
}
```

### Understanding 
```{r}
head(weather[, 3:4])
new_locs <- weather[, 3:4] + matrix(c(0.1, 0.1), ncol = 2, nrow = nrow(weather), byrow = T)
head(new_locs)

X <- rbind(new_locs, weather[, 3:4])
str(X) # df

str(as.matrix(X))    # [1:314, 1:2] new and old locs
str(RFearth2dist(as.matrix(X))) # 'dist' num [1:49141] 697 501 501 531 623 ...
# the dist btw any 2 pts in cartesian system

str(as.matrix(RFearth2dist(as.matrix(X))))  # num [1:314, 1:314] 0 697 501 501 531

 
```


Next, we define the likelihood funciton; 
note that the parameter definitions for the parsimonious Matérn are different than for the conditional approach.

```{r}
loglik_sh_pars_model <- function(theta, shift = F) {
  # theta1: delta1
  # theta2: delta2
  # theta3: kappa
  # theta4: nu1
  # theta5: nu2
  # theta6: rho
  # theta7: tau1
  # theta8: tau2/100
  # theta9: sigam2_11
  # theta10: sigma2_22/10000
  
  if (theta[3] < 0.00001 | theta[4] < 0.1 | theta[5] < 0.1 | 
      abs(theta[6]) >= 1 | theta[7] <= 0 | theta[8] <= 0 |
      theta[9] <= 0 | theta[10] <= 0) {
    return(Inf)
  } else if (abs(theta[6]) > sqrt(theta[4] * theta[5]) / (0.5 * (theta[4] + theta[5]))) {
        return(Inf)
  } else {
        
    if (!shift) theta[1] <- theta[2] <- 0
    
    # convert theta[8] and theta[10] back, they were divided by 100, 10000
    theta[8] <- theta[8] * 100
    theta[10] <- theta[10] * 10000
    
    # get the data
    Z <- form_Z(1L, scale = F) # model_num = 1
    
    # get the cross-covariance matrice
    Matrices <- sh_pars_mats(theta)
    S <- Matrices$SY + Matrices$So
    cholS <- chol(S)
    
    # loglik
    loglik <- -(-0.5 * nrow(Z) * log(2 * pi) 
                - 0.5 * determinant(S)$modulus   ## ??
                - 0.5 * t(Z) %*% chol2inv(cholS) %*% Z) %>% as.numeric()
    
    return(loglik)
    
  }
 
}
```



Finally, we estimate the parameters.

```{r}
## Set shift=TRUE below to estimate the shift parameters; 
## otherwise they are fixed to zero
## and we get the same likelihood and parameter estimates of Gneiting (2010)

optim_est_sh_pars <- optim(par = c(0.5, -1, 1/95.88, 0.6, 1.6, -0.5, 0.019, 
                                   69.66/100, 6.95, 67191/10000),
      fn = loglik_sh_pars_model, shift = T,
      control = list(trace = 6, maxit = 3000))
```

```{r}
save(optim_est_sh_pars, file = "./Results/
     Shifted_Pars_est_results.rda")
```

```{r}

print(paste0("Log-lik. of shifted parsimonious model is ",  -optim_est_sh_pars$value))

print(paste0("AIC of the shifted parsimonious model is ", 
             optim_est_sh_pars$value * 2 + 2 * 10)) # 10 pars

```


For cokriging and for cross-validation we follow the same approach as earlier.

```{r}
theta_lk <- optim_est_sh_pars$par
theta_lk[8] <- theta_lk[8] * 100
theta_lk[10] <- theta_lk[10] * 10000

X <- sh_pars_mats(theta = theta_lk)

## loop over each obs
print("Running CV for shifted parsimonious model...")
pred_sh_pars <- foreach(i = 1:m1, .combine = "rbind") %do% {
  X <- sh_pars_mats(theta = theta_lk)
  cbind(cokrig(X, i = c(i, i + m1)), model_num = 1)
}
```


## Processing the results

The following code is only documented in-line since it just involves data manipulation for obtaining the results shown in the paper. 
For verification we find the mean absolute error (MAE), the root mean-squared prediction error (RMSPE) and the continuous probability rank score [CRPS, @Gneiting_2005].

```{r}
### Analyse results

## put data set into long format
str(Dobs) # num [1:157, 1:157] 0 697 502 502 532


a <- apply(Dobs, 1, function(x) sort(x)[1:2])
str(a) # num [1:2, 1:157] 0 417 0 19 0 ...
a[1:2, 1:5]


b <- apply(Dobs, 1, function(x) sort(x))
str(b)
b[1:5, 1:5]

c <- apply(Dobs, 1, function(x) sum(sort(x)[1:2]))
str(c)  #  Named num [1:157] 417.5 19 48.1 45.1 34.1 ..
c[1:5]

```


```{r}
head(Dobs)
```
```{r,message=FALSE}
### Analyse results

## put data set into long format
weather_long <- mutate(weather,loc_num = 1:nrow(weather)) %>%
  mutate(sum_D = apply(Dobs,1,function(x) sum(sort(x)[1:2]))) %>%
  gather(process,z,temperature,pressure,convert = TRUE)


## Utility wrapper around the crps function
crps_wrapper <- function(Z,mu,sd) {
  crps(obs = Z, pred = cbind(mu,sd))$crps
  
  # obs: A vector of observations.
  # pred: A vector or matrix of the mean and standard deviation of a normal distribution
  # crps: Continous ranked probability scores
  # CRPS: Mean of crps
}


```


#### understanding 
```{r}
head(mutate(weather, loc_num = 1:nrow(weather)) %>%
       mutate(sum_D = apply(Dobs, 1, function(x) sum(sort(x)[1:2]))))

```

```{r}
head(mutate(weather,loc_num = 1:nrow(weather)) %>%
       mutate(sum_D = apply(Dobs, 1, function(x) sum(sort(x)[1:2]))) %>%
       gather(key = "process", value = "z", temperature, pressure, convert = T)
       #gather(process, z, temperature, pressure, convert = T)
       )

```

```{r}
str(pred)
head(pred, 30)
tail(pred, 50)
```

```{r}
sanitize_results <- function(pred) {
  
  ## put dataset into long format
  pred2 <- mutate(pred,                                          # take LOOCV pred
                  process = ifelse((model_num < 5 & i <= m1) |   # assign row name
                                     (model_num >= 5 & i > m1), "temp", "pressure"),
                  loc_num = ((i - 1) %% m1) + 1) %>%             # add loc_num
    left_join(weather_long) %>%                                  # join in weather long
    dplyr::select(-i, -z)
  
  
  results <- pred2 %>%
    group_by(process, model_num) %>%                             # group by process, mod
    summarise(MAE = mean(abs(mu_pred - Z)),                      # Mean absolute error
              MAE_se = sd(abs(mu_pred - Z)) / sqrt(m1 - 1),      # s.e. of MAE
              Bias = mean(mu_pred - Z),                          # bias
              Bias_se = sd(mu_pred - Z) / sqrt(m1 - 1),          # s.e. of bias
              Bias_normalized  = mean(mu_pred - Z) / sqrt(var_pred),  # normalized by sd
              RMSPE = sqrt(mean((mu_pred - Z)^2)),                    # root MS pred E
              CRPS = mean(crps_wrapper(Z, mu_pred, sqrt(var_pred))),  # mean of crps
              CRPS_se = sd(crps_wrapper(Z, mu_pred, sqrt(var_pred))) / sqrt(m1 - 1))
  
  results

}

results <- sanitize_results(pred = pred)
print(results)
```


Next we do the same for the shifted parsimonious Matérn model.

```{r}
str(pred_sh_pars) 
```

```{r}

sh_pars_results <- sanitize_results(pred_sh_pars) %>%
  dplyr::select(model_num, MAE, RMSPE, CRPS)

sh_pars_results$model_num <- "Shifted_Parsim"
head(sh_pars_results)
```










