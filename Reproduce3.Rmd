---
title: "Reproduc3_(M4 only)"
author: "XC"
date: "21/10/2021"
output: pdf_document
---

```{r}
library(INLA)    # for triangualtion
library(deldir)  # for tessalation area

# for core operation
library(dplyr)
library(tidyr)
library(Matrix)

library(ggplot2)
library(gridExtra)
library(grid)
library(extrafont)


library(maptools)
library(mapproj)
library(RandomFields)


library(verification)

library(foreach)
library(doParallel)


library(bicon)
```



## The data

Available from package `RandomFields`
```{r}
data("weather", package = "RandomFields")
str(weather)  # num [1:157, 1:4] 

weather <- weather %>% data.frame()
weather %>% head()
```


The `weather` table contains four fields:
  - latitude, 
  - longitude, 
  - pressure forecasting errors,
  - temperature forecasting errors 
  for December 13, 2003 at 4 p.m. in the North American Pacific Northwest.


Since pressure and temperature have different units, we find a scaling factor by taking the ratio of the sample variances of the two variates, and computing its square root.

We will use this factor to scale the pressure variable.

```{r}
Press_scale <- var(weather$pressure) / var(weather$temperature) %>% 
  sqrt() %>% 
  as.numeric()
```


From this data frame we extract $Z_1$ and $Z_2$ and concatenate them into one long vector $Z$ through a function `Form_Z`.


```{r}
Form_Z <- function(model_num, scale = T) {
  Z1 <- matrix(weather$temperature)
  Z2 <- matrix(weather$pressure)
  
  if (scale) Z2 <- Z2 / Press_scale
  
  if (model_num != 1) {
     temp <- Z1
     Z1 <- Z2
     Z2 <- temp
  }
  
  Z <- rbind(Z1, Z2)
}
```


We also define `m1` as the number of observations of $Z_1$, `m2` as the number of observations of $Z_2$ and `m` as the total number of observations.

```{r}
m1 <- m2 <- nrow(weather)
m <- m1 + m2  # total obs

#I_m1<- Diagonal(m1)
```


## Process Discretisation

- The process of each variable is a curved flunctuated suface. Follow the FEM idea, We approximate this suface by using discretized simple geometry subdomains, e.g. triangles, which are termed as "finite elements",  and the process over each subdomain can itself be approximated by a 2D planar surface. 

- Relationship btw delauny triangulation and Voronoi tesselation?
  - Since each triangle has to be determined by 3 nodes, we seek a more efficient way to deternime a simple geometry in the fashsion that connecting the bisector of each pairs of the tri nodes to create the Voronoi tesselation, and by the property of triangles, each node is the centriod of the hexagon (tesselation), so using the tesselation as the finite element and with known centroids, its area is much easier to determine.
  


Each node of triangle is the center point of the V. tesselation, so the area of the tess is easier to calculate than that of the triangules of irregualar length and angles. 


We approximate the processes as a sum of elemental basis functions (tent functions) constructed on a triangulation. 

The triangulation is formed using the mesher in the `INLA` package.

while `initFEbasis`, which takes information from the `INLA` mesher and casts it into a `Mesh` object and provide several useful methods associated with the `Mesh` class for plotting later on as well as the information on the areas of the elements in the Voronoi tesselation, which will be used to approximate the integrations.

Alternatively, we use `deldir` package to 1. construct the voroini tesselation using the triangualtion mesh nodes from INLA; 2. extract the tessalation area directly from the output summary. 


```{r}
## discretizing process Y1,Y2 using triangular grid

# constructing mesh

#str(weather[c("lon", "lat")])  'data.frame':	157 obs. of  2 variables:

mesh <- inla.mesh.2d(loc = weather[c("lon", "lat")], 
             cutoff = 0, 
             max.edge = 0.75,
             offset = 4)



# mesh locations
str(mesh$loc)  # num [1:2071, 1:3]
mesh_locs <- mesh$loc[, 1:2]

```


```{r}
## compute distances as in Gneiting(2010) -- greate-circle distance

### Greate circle distance: shortest distance between two points on the surface of a sphere, measured along the surface of the sphere not interior

# tranform coords from ellipsoid earth to cartesian
# cartesian dist btw pairs of locs
D <- as.matrix(RFearth2dist(coord = as.matrix(mesh_locs)))  
D_vec <- as.double(c(D))   # num [1:4289041] = 2071*2071


## obs locations in cartesian 
Dobs <- as.matrix(RFearth2dist(coord = as.matrix(weather[c("lon", "lat")])))
Dobs_vec <- c(Dobs)

```


```{r}
## construct voroini tesselation and then get the area
Voronoi <- deldir(x = mesh_locs[, 1], 
       y = mesh_locs[, 2], 
       plot = T, 
       rw = c(min(mesh_locs[, 1]) - 0.0001,
              max(mesh_locs[, 1]) + 0.0001,
              min(mesh_locs[, 2]) - 0.0001,
              max(mesh_locs[, 2]) + 0.0001))

str(Voronoi)

Area_tess <- Voronoi$summary$dir.area


Mesh_df <- data.frame(Lon = mesh_locs[, 1], Lat = mesh_locs[, 2],
           Area_tess = Area_tess, Indx = 1:length(mesh_locs[, 1]))

head(Mesh_df)

str(Mesh_df[, "Area_tess"])  # num[1:2071]
str(Mesh_df["Area_tess"])    # df
```



## Establish the dimension of our grid
Since we will be evaluating $Y_1$ and $Y_2$ on the same grid, `n1` = `n2`.

```{r}
## mesh size
n1 <- n2 <- nrow(mesh_locs)
n <- n1 + n2   # # 4142
```



When using finite elements, this reduces to the integrand being evaluated at the Delauny triangulation nodes times the area of the corresponding V tess with each tri node as its center points. 

```{r}
#str(mesh_locs[1, ])  # num [1:2] -129.6 36.8
#str(t(t(mesh_locs) - mesh_locs[1, ]))


h <- matrix(0, n1 * n2, 2)
Areas <- rep(0, n1 * n2)
for (i in 1:n2) {
  h[((i - 1) * n1 + 1) : (i * n1), ] <- t(t(mesh_locs) - mesh_locs[1, ])
  Areas[((i - 1) * n1 + 1) : (i * n1)] <- Mesh_df[, "Area_tess"]
}

# h and Areas will be used in constructing bisquare_B function for B

```



## Organising the observations

- Construct an incidence matrix to map the obs to process node
  - with `1` wherever obs coincides with process and `0` otherwise
  
- The dimension of this incidence matrix is `m1 + m2` by `n1 + n2`
  - where the `m1, m2` are the number of obs
  
- As Temp and Press have collocated obs, only need to find incidence matrix for Z1,
then the whole incidence matrix using `bdiag` with dimension as mentioned above

- Use `left_join` function to find the points where obs coincide with proc nodes, which returns `NA` if not

```{r}
str(mesh_locs)  #  num [1:2071, 1:2] -130 -114 
# Need to df it
head(weather)

mesh_locs <- data.frame(lon = mesh_locs[, 1], lat = mesh_locs[, 2]) # lower case

idx <- which(!is.na(left_join(mesh_locs, weather)$temperature))
str(idx)   #  int [1:157] 9 10 11 12 13 14 15 16 17 18 ...

C1 <- sparseMatrix(i = 1:m1, j = idx, x = 1, dims = c(m1, n1)) # incidence matrix of Z1

C <- bdiag(C1, C1)    # incidence matrix of Z1, Z2
```



## Maximum likelihood estimation

As we only have 1 model and its reverse, and they are of the same length, so no need to append theta for shorter length of parameter vectors of some simpler models. 

Next, we write a function that given the model number and parameters will return the desired matrices `SY` and `So`. 

Then add `SY` and `So` together to obtain the joint covariance matrix for the joint obs `Y1` and `Y2` which is the same as `Z1` and `Z2` at the obs locations.

If `whole_mesh` is T, and the process covariance matrix is evaluated at the all process locations (entire mesh), which is used for cokriging at unobsered locations. 


### Construct Covariance Matrices in the log_lik
```{r}

construct_matrices <- function(theta, model_num, whole_mesh = F) {
  
  B <- theta[9] * bisquare_B(h1 = h[, 1], h2 = h[, 2],
                  delta = theta[11:12], r = theta[10],
                  n1 = n1, n2 = n2,
                  areas = Areas)
  
  C1B <- C1 %*% B
  
  ## Form matrices (scaled pressure)
  
  S11 <- makeS(r = Dobs_vec, var = theta[3], kappa = theta[5], nu = theta[7])
  S2_1 <- makeS(r = Dobs_vec, var = theta[4], kappa = theta[6], nu = theta[8])
  
  
  if (model_num %in% c(1, 2) | whole_mesh == T) {
    
    S11_proc <- makeS(r = D_vec, var = theta[3], kappa = theta[5], nu = theta[7])
    S12 <- (C1 %*% S11_proc) %*% t(C1B)
    S21 <- t(S12)
    S22 <- S2_1 + forceSymmetric(C1B %*% forceSymmetric(S11_proc) %*% t(C1B))
  }
  
  
  if (whole_mesh == T) {
    S2_1 <- makeS(r = D_vec, var = theta[4], kappa = theta[6], nu = theta[8])
    
    S11 <- S11_proc
    S12 <-  S11_proc %*% t(B)
    S21 <- t(S12)
    S22 <- S2_1 + crossprod(chol(S11_proc) %*% t(B))
  }
  
  
  ## Form matrices (Unscaled pressure)
  S11_true <- ifelse (model_num == 1, 1, P_scale^2) * S11
  S22_true <- ifelse (model_num == 1, P_scale^2, 1) * S22
  S12_true <- S12 * P_scale
  S21_true <- S21 * P_scale
  
  SY_true <- rbind(cbind(S11_true, S12_true), cbind(S21_true, S22_true)) %>% as("dgeMatrix")
  
  
  #Imat <- function(n) {
  #  sparseMatrix(i = 1:n, j = 1:n, x = 1)
  #}
  source("SparseMat_Imat.R")

  I_m1 <- Imat(m1)
  So_11_true <- ifelse(model_num == 1, 1, P_scale^2) * theta[1] * I_m1
  So_22_true <- ifelse(model_num == 1, P_scale^2, 1) * theta[2] * I_m1
  
  So_true <- bdiag(So_11_true, So_22_true)
  

  if (whole_mesh == T) {
    So_true <- t(C) %*% So_true %*% C
  }
  
  
  list(SY = SY_true, So = So_true, Z = Form_Z(model_num, scale = F))
 
}
```


### Define log_likelihood
  - Usual Gaussian log-likelihood function
  - Also allow for dropping certain obs for CV
  - The obs we wish to drop is stored in parameter `i`. and if `i = NULL`, no obs is dropped. This arg is useful for CV
  
```{r}

loglik_Mod <- function(theta, model_num, i = NULL) {
  
  # theta1: sigma2e1
  # theta2: sigma2e2
  # theta3: sigma211
  # theta4: sigma22_1
  # theta5: kappa11
  # theta6: kappa2_1
  # theta7: nu11
  # theta8: nu2_1
  # theta9: A
  # theta10: r
  # theta11: d1
  # theta12: d2
  
  ## Hard constraints on parameters
  if (theta[1] <= 0 | theta[2] <= 0 | theta[3] <= 0 | theta[4] <= 0
      | theta[5] <= 0.001 | theta[6] <= 0.001 | theta[7] <= 0.05 
      | theta[8] <= 0.05 | theta[10] <= 0.0005) {
    return(Inf)
  } else {
    ## Construct matrics
    X <- construct_matrices(theta, model_num)
  
    ## Drop obs for CV
    if (is.null(i)) {
      SY <- X$SY
      So <- X$So
      Z <- X$Z
    } else {
      SY <- X$SY[-i, -i]
      So <- X$So[-i, -i]
      X$Z[-i, , drop = F]
    }    
    
    ## Evaluate log_likilihood function
    source("log_det.R")
    
    cholYo <- chol(SY + So)
    log_lik <- -(-0.5 * nrow(Z) * log(2 * pi) 
                 - 0.5 * log_det(cholYo)
                 - 0.5 * t(Z) %*% chol2inv(cholYo) %*% Z) %>% as.numeric()
    
    
    ## return
    return(log_lik)
    
  }
 
}

```



### Optimization
- Use Â® function `optim` (BFGS)
- Allow for 3000 maximum iterations; trace = 6; Hessian is not required; 
- `i` indicate the indices we wish to drop; if `i = NULL` then all obs are included.
- `optim_loglik` function will be used on different models later

```{r}
optim_loglik <- function(par, model_num, i = NULL) {
  optim(par = par, 
        fn = loglik_Mod,
        i = i,
        hessian = F, 
        model_num = model_num,
        control = list(trace = 6,
                       pgtol = 0, 
                       parscale = rep(0.1, length(par)),
                       maxit = 3000))
  
}
```


```{r}

fit_m1 <- optim_loglik(par = c(0.01, 1, 5, 15, 0.01, 0.01, 0.6, 1.5, -0.2, 0.1, 0, 0),
                model_num = 1, i = NULL)

fit_m2 <- optim_loglik(par = c(1, 0.01, 15, 5, 0.01, 0.01, 1.5, 0.6, -0.2, 0.1, 0, 0),
             model_num = 2, i = NULL)
```